{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4567ba",
   "metadata": {},
   "source": [
    "### 3.1 The world as floating-point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730082e",
   "metadata": {},
   "source": [
    "由于浮点数是网络处理信息的方式，我们需要一种方法来编码我们想要处理的那种真实世界的数据，使其可以被网络消化，然后将输出解码回我们可以理解并用于我们的目的的东西"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39246f",
   "metadata": {},
   "source": [
    "深度神经网络通常分阶段学习从一种形式的数据到另一种形式的转换，这意味着每个阶段之间部分转换的数据可以被认为是一系列中间表示。对于图像识别，早期表示可以是边缘检测或某些纹理(如皮毛)。更深层的表征可以捕捉到更复杂的结构，比如耳朵、鼻子或眼睛。\n",
    "<br>一般来说，这种中间表示是浮点数的集合，它们表征输入并以一种有助于描述输入如何映射到神经网络输出的方式捕获数据结构。这种特征是特定于手头的任务，并从相关示例中学习。这些浮点数集合及其操作是现代人工智能的核心——我们将在本书中看到几个这样的例子。\n",
    "<br>重要的是要记住，这些中间表示(如图3.1的第二步所示)是将输入与前一层神经元的权重相结合的结果。每个中间表示对于之前的输入都是唯一的。\n",
    "<br>在我们开始将数据转换为浮点输入之前，我们必须首先对PyTorch如何处理和存储数据(作为输入、中间表示和输出)有一个坚实的理解。本章将专门讨论这一点。\n",
    "<br>为此，PyTorch引入了一个基本的数据结构:张量。在第2章中，当我们在预训练的网络上进行推理时，我们已经碰到了张量。对于那些来自数学、物理或工程的人来说，张量这个术语与空间、参照系和它们之间的变换的概念捆绑在一起。不管是好是坏，这些概念在这里并不适用。在深度学习的背景下，张量是指将向量和矩阵泛化到任意数量的维度，如图3.2所示。同一概念的另一个名称是多维数组。张量的维数与用来指代张量内标量值的索引数一致。\n",
    "<br>PyTorch并不是唯一处理多维数组的库。NumPy是迄今为止最流行的多维数组库，它现在可以说已经成为数据科学的通用语言。PyTorch具有与NumPy的无缝互操作性，它带来了与Python中其他科学库的一流集成，例如SciPy (www.scipy.org)， Scikit-learn (https://scikit-learn).org)和Pandas (https://pandas.pydata.org)。\n",
    "<br>与NumPy数组相比，PyTorch张量有一些超能力，比如在图形处理单元(gpu)上执行非常快速的操作，在多个设备或机器上分配操作，并跟踪创建它们的计算图。这些都是实现现代深度学习库时的重要特性。\n",
    "<br>我们将从介绍PyTorch张量开始本章，涵盖基础知识，以便为本书其余部分的工作奠定基础。首先，我们将学习如何使用PyTorch张量库来操作张量。这包括数据如何存储在内存中，如何在常量时间内对任意大的张量执行某些操作，以及前面提到的NumPy互操作性和GPU加速。如果张量要成为我们编程工具箱中的必备工具，那么理解它的功能和API是很重要的。在下一章中，我们将很好地运用这些知识，并学习如何用神经网络学习的方式表示几种不同类型的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93ae84",
   "metadata": {},
   "source": [
    "### 3.2 Tensors: Multidimensional arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80c0ae3",
   "metadata": {},
   "source": [
    "我们已经了解到张量是PyTorch的基本数据结构。张量是一个数组:也就是说，它是一种数据结构，存储了一组数字，这些数字可以使用索引单独访问，并且可以使用多个索引进行索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd96f79",
   "metadata": {},
   "source": [
    "### 3.3  From Python lists to PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb421bfc",
   "metadata": {},
   "source": [
    "让我们看看列表索引的作用，这样我们就可以把它和张量索引进行比较。取Python中的三个数字列表(.code/p1ch3/1_tensors.ipynb):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029f183",
   "metadata": {},
   "source": [
    "#### 3.2.1 From Python lists to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436b71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "231013af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 1.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1.0, 2.0, 1.0]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7d43e",
   "metadata": {},
   "source": [
    "#### 3.2.2 Constructing our first tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d957af07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]), torch.Size([3]), torch.Size([3]), torch.float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(3)\n",
    "a, a.shape, a.size(), a.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43678ae7",
   "metadata": {},
   "source": [
    "#### 3.2.3 The essence of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad90a47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([4.0, 1.0, 5.0, 3.0, 2.0, 1.0])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f96bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1ff70",
   "metadata": {},
   "source": [
    "### 3.3 Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54bef2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:] # All rows after the first; implicitly all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c3bbd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, :] # All rows after the first; all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0508c974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, 0] # All rows after the first; first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "464009a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[None] #Adds a dimension of size 1, just like unsqueeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d103cb",
   "metadata": {},
   "source": [
    "### 3.4 Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59030956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4790, -0.3357,  0.0369, -0.8018, -0.2732],\n",
       "         [-0.9490,  0.2856, -0.0544, -0.6832, -0.0039],\n",
       "         [-0.4761, -0.4959, -0.5438, -0.8931,  1.4455],\n",
       "         [ 0.2999, -2.4977,  0.0224, -1.0825,  1.7776],\n",
       "         [-1.5450,  0.2057,  1.9779,  0.5774, -0.4647]],\n",
       "\n",
       "        [[-0.6520, -1.3101,  0.0547, -1.1023, -0.5191],\n",
       "         [-0.2518,  0.0741, -0.2725,  1.2209, -1.4706],\n",
       "         [-1.7543,  0.3093, -0.3433, -0.4226,  0.9534],\n",
       "         [ 0.9407, -0.0685, -0.4802, -0.2376,  0.3496],\n",
       "         [-0.3828,  0.4871,  1.1053, -0.7178, -0.3749]],\n",
       "\n",
       "        [[ 0.7069, -0.1460,  0.4389,  0.5662, -0.6980],\n",
       "         [-1.5622, -0.4604,  0.6434, -0.8334,  0.5831],\n",
       "         [ 1.1799,  0.8006,  1.1330,  0.5165, -0.0722],\n",
       "         [ 1.2607,  0.0654, -0.5193,  0.6160, -1.9186],\n",
       "         [-0.8530, -0.4830,  1.2078, -0.8062,  0.7608]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape [channels, rows, columns]\n",
    "img_t = torch.randn(3, 5, 5)\n",
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "464af256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce9a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape [batch, channels, rows, columns]\n",
    "batch_t = torch.randn(2, 3, 5, 5)\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd61ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1414, -0.5973,  0.1769, -0.4460, -0.4968],\n",
       "         [-0.9210, -0.0336,  0.1055, -0.0986, -0.2971],\n",
       "         [-0.3502,  0.2047,  0.0820, -0.2664,  0.7755],\n",
       "         [ 0.8337, -0.8336, -0.3257, -0.2347,  0.0695],\n",
       "         [-0.9269,  0.0700,  1.4304, -0.3155, -0.0263]]),\n",
       " tensor([[[ 0.5902, -0.6111,  0.4027, -0.4749, -0.8452],\n",
       "          [ 0.1349,  0.1637,  0.2385, -0.4682,  0.4454],\n",
       "          [-0.1025, -0.3225,  0.1738,  0.2935, -0.2404],\n",
       "          [-0.0469, -0.0194,  0.8854, -0.3621,  0.4169],\n",
       "          [ 0.0712, -0.5430,  0.6846,  0.2139, -0.0580]],\n",
       " \n",
       "         [[-0.4651, -1.3302, -0.8736,  1.9278,  0.2871],\n",
       "          [-0.2794, -0.6624, -0.2395, -0.0164,  0.3041],\n",
       "          [-0.1466, -0.0037, -0.5810,  0.0579,  0.5744],\n",
       "          [ 0.5752,  0.9839, -0.0535, -0.7527, -0.0159],\n",
       "          [ 0.4237,  0.4090, -0.7972,  0.5511,  0.0677]]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算平均值\n",
    "imgage_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "imgage_gray_naive, batch_gray_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b9af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb97e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7daa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383a27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4c36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f51d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c078a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7802c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlwpt)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
